{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "MAIN_MODULE_PATH = os.path.join(os.getcwd(), '..', '..')\n",
    "sys.path.append(MAIN_MODULE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightfm.data\n",
    "import lightfm.cross_validation\n",
    "import lightfm.evaluation\n",
    "import pycountry\n",
    "\n",
    "from defaults import BOOK_RATINGS, BOOKS, USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-patrick",
   "metadata": {},
   "source": [
    "# Preparing data\n",
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings(path=BOOK_RATINGS):\n",
    "    ratings = pd.read_csv(BOOK_RATINGS, sep=';')\n",
    "    ratings['Book-Rating'] = ratings['Book-Rating'].astype('int8')\n",
    "    return ratings\n",
    "\n",
    "def load_books(path=BOOKS):\n",
    "    books = pd.read_csv(path, sep=';', error_bad_lines=False, index_col='ISBN')    \n",
    "    books = books.loc[pd.to_numeric(books['Year-Of-Publication'], errors='coerce').dropna().index]\n",
    "    books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(\"int8\")\n",
    "    return books\n",
    "\n",
    "def load_users(path=USERS):\n",
    "    return pd.read_csv(path, sep=';', index_col='User-ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_BOOK_RATINGS = 20\n",
    "\n",
    "def preprocess_ratings(ratings: pd.DataFrame, books: pd.DataFrame, users: pd.DataFrame,\n",
    "                   min_book_ratings: int = MIN_BOOK_RATINGS):\n",
    "    books_ratings_joined = pd.merge(ratings, books, left_on='ISBN', right_index=True, how='left')\n",
    "    books['n_ratings'] = books_ratings_joined.groupby('ISBN')['Book-Rating'].size()\n",
    "    popular_books = books[books['n_ratings'] > min_book_ratings]\n",
    "    \n",
    "    ratings = pd.merge(popular_books, ratings, left_index=True, right_on='ISBN', how='left')\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, books, users = load_ratings(), load_books(), load_users()\n",
    "ratings = preprocess_ratings(ratings, books, users)[['ISBN', 'User-ID', 'Book-Rating']]\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ratings), len(books), len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ratings['Book-Rating']>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-costume",
   "metadata": {},
   "source": [
    "# SVD surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "from surprise import SVD, SVDpp, NMF, NormalPredictor\n",
    "from surprise import Dataset, Reader, \n",
    "from surprise.model_selection import cross_validate, RandomizedSearchCV, train_test_split\n",
    "from surprise.accuracy import mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load_from_df(ratings, Reader(rating_scale=(1, 10)))\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=17)\n",
    "model = SVDpp()\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fitted.test(test)\n",
    "precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(precisions.values()), mean(recalls.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(predictions), mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_prec_rec = pd.DataFrame.from_dict(precisions, orient='index', columns=['precision']).join(pd.DataFrame.from_dict(recalls, orient='index', columns=['recall']))\n",
    "# users_prec_rec.index = users_prec_rec.index.astype(int)\n",
    "# users_prec_rec = users_prec_rec.join(users)\n",
    "# users_prec_rec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(17)\n",
    "top_n_recommendations = {user: recommendations for user, recommendations in get_top_n(predictions, n=5).items() if len(recommendations) > 3}\n",
    "random_n = random.choices(list(top_n_recommendations.items()), k=10)\n",
    "for user_id, user_recommendations in random_n:\n",
    "    print(users.loc[int(user_id)].to_dict())\n",
    "    for i, (book_id, rank) in enumerate(user_recommendations):\n",
    "        print(f\"{i}. [Rank={rank}], Title:, {books.loc[book_id].to_dict()['Book-Title']}\")\n",
    "#         pprint.pprint(books.loc[book_id].to_dict(), indent=4)\n",
    "    print('\\n-'.rjust(30, '-'), end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-canberra",
   "metadata": {},
   "source": [
    "# Hybrid model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lightfm.data.Dataset()\n",
    "dataset.fit(ratings['ISBN'].unique(), ratings['User-ID'].unique())\n",
    "interactions, weights = dataset.build_interactions(ratings.itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = lightfm.cross_validation.random_train_test_split(interactions, test_percentage=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lightfm.LightFM(no_components=10)\n",
    "fitted_model = model.fit(train, epochs=64, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_per_user = lightfm.evaluation.precision_at_k(fitted_model, test_interactions=test, train_interactions=train, k=5)\n",
    "np.mean(prec_per_user)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendations",
   "language": "python",
   "name": "recommendations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
