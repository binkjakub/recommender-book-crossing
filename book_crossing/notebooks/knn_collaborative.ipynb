{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "MAIN_MODULE_PATH = os.path.join(os.getcwd(), '..', '..')\n",
    "sys.path.append(MAIN_MODULE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightfm.data\n",
    "import lightfm.cross_validation\n",
    "import lightfm.evaluation\n",
    "\n",
    "from defaults import BOOK_RATINGS, BOOKS, USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-hepatitis",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings(path=BOOK_RATINGS):\n",
    "    ratings = pd.read_csv(BOOK_RATINGS, sep=';')\n",
    "    ratings['Book-Rating'] = ratings['Book-Rating'].astype('int8')\n",
    "    return ratings\n",
    "\n",
    "def load_books(path=BOOKS):\n",
    "    books = pd.read_csv(path, sep=';', error_bad_lines=False, index_col='ISBN')    \n",
    "    books = books.loc[pd.to_numeric(books['Year-Of-Publication'], errors='coerce').dropna().index]\n",
    "    books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(\"int8\")\n",
    "    return books\n",
    "\n",
    "def load_users(path=USERS):\n",
    "    return pd.read_csv(path, sep=';', index_col='User-ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_BOOK_RATINGS = 20\n",
    "\n",
    "def preprocess_ratings(ratings: pd.DataFrame, books: pd.DataFrame, users: pd.DataFrame,\n",
    "                   min_book_ratings: int = MIN_BOOK_RATINGS):\n",
    "    books_ratings_joined = pd.merge(ratings, books, left_on='ISBN', right_index=True, how='left')\n",
    "    books['n_ratings'] = books_ratings_joined.groupby('ISBN')['Book-Rating'].size()\n",
    "    popular_books = books[books['n_ratings'] > min_book_ratings]\n",
    "    \n",
    "    ratings = pd.merge(popular_books, ratings, left_index=True, right_on='ISBN', how='left')\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, books, users = load_ratings(), load_books(), load_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ratings), len(books), len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = preprocess_ratings(ratings, books, users)[['ISBN', 'User-ID', 'Book-Rating']]\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-belle",
   "metadata": {},
   "source": [
    "# KNNsurprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import cross_validate, KFold, train_test_split\n",
    "\n",
    "data = Dataset.load_from_df(ratings[['ISBN', 'User-ID', 'Book-Rating']], Reader(rating_scale=(1, 10)))\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=RANDOM_STATE)\n",
    "\n",
    "results = []\n",
    "\n",
    "for sim_measure_name in ['cosine', 'msd', 'pearson']:\n",
    "    algo = KNNBasic(sim_options = {'name': sim_measure_name})\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "    precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "    \n",
    "    results.append({'similarity_measure': sim_measure_name, 'precision': precision, 'recall': recall})\n",
    "    \n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:,.2f}'.format):\n",
    "    display(results.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs",
   "language": "python",
   "name": "rs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
